# -*- coding: utf-8 -*-
"""model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q3f15gvs1E10w_EnAmAmTlp_tdBIh3hI

# No correr ya esta
"""

!pip install tensorflow

!git clone https://github.com/maximocostaroman/Trabajo-Practico-Integrador-Redes-Neuronales-Profundas.git



!unzip /content/Trabajo-Practico-Integrador-Redes-Neuronales-Profundas/archive.zip

import os
import csv
import shutil
import random

# Carpetas de entrada
original_dirs = ['train', 'test']  # carpetas originales

# Carpetas de salida
base_output_dir = 'images'
train_output_dir = os.path.join(base_output_dir, 'train')
test_output_dir = os.path.join(base_output_dir, 'test')

# Crear carpetas de salida
os.makedirs(train_output_dir, exist_ok=True)
os.makedirs(test_output_dir, exist_ok=True)

# CSV de salida
csv_output = 'archive_labels.csv'

# Recolectar imágenes desde carpetas originales
all_data = []

for split in original_dirs:
    for label in os.listdir(split):
        label_path = os.path.join(split, label)
        if not os.path.isdir(label_path):
            continue
        for i, img_name in enumerate(os.listdir(label_path)):
            src_path = os.path.join(label_path, img_name)
            # Crear nombre único (ignora extensión original y usa .png)
            new_filename = f"{split}_{label}_{i:05d}.png"
            all_data.append((src_path, new_filename, label))  # (ruta_origen, nombre_nuevo, etiqueta)

# Mezclar aleatoriamente
random.shuffle(all_data)

# Dividir en train y test (80/20 por defecto)
split_ratio = 0.8
split_index = int(len(all_data) * split_ratio)
train_data = all_data[:split_index]
test_data = all_data[split_index:]

# Preparar CSV
with open(csv_output, mode='w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['filename', 'label', 'split'])

    # Guardar y copiar imágenes para TRAIN
    for src_path, new_filename, label in train_data:
        dst_path = os.path.join(train_output_dir, new_filename)
        shutil.copy(src_path, dst_path)
        writer.writerow([os.path.join('train', new_filename), label, 'train'])

    # Guardar y copiar imágenes para TEST
    for src_path, new_filename, label in test_data:
        dst_path = os.path.join(test_output_dir, new_filename)
        shutil.copy(src_path, dst_path)
        writer.writerow([os.path.join('test', new_filename), label, 'test'])

"""# **CORRER A PARTIR DE ACA!!**"""

import torch
from torch.utils.data import Dataset
from torchvision.io import read_image
from torchvision import tv_tensors
from torchvision.transforms import v2
from torchvision import transforms
import torchvision.transforms.functional as TF
import torch.nn.functional as F
from PIL import Image
import json
import pandas as pd
import numpy as np
import os

class ImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir,
                 mode= "train",split= "train" , transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform

        self.img_labels = self.img_labels[self.img_labels['split'] == split].reset_index(drop=True)
        unique_labels = sorted(self.img_labels['label'].unique())
        self.label_map = {label: i for i, label in enumerate(unique_labels)}
        self.num_classes = len(unique_labels)
        print(f"Label mapping: {self.label_map}")


    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_filename_in_csv = self.img_labels.iloc[idx]['filename']
        img_path = os.path.join(self.img_dir, img_filename_in_csv)
        image = Image.open(img_path).convert('L')  # Asegura que esté en modo grayscale

        image = TF.to_tensor(image)
        # Convertir la etiqueta de cadena a su índice numérico
        label_str = self.img_labels.iloc[idx]['label']
        label = self.label_map[label_str]
        # Convertir el índice a un tensor de tipo long
        label = torch.tensor(label, dtype=torch.long)

        return image, label

# Transformaciones
import torch
from torchvision.transforms import v2

train_transforms = v2.Compose([
    v2.Resize(size=(48,48)),
    # v2.Grayscale(num_output_channels=3),
    v2.ToTensor(),
    v2.ToDtype(dtype=torch.float32, scale=True),
    v2.RandomHorizontalFlip(p=0.5),
    v2.RandomVerticalFlip(p=0.5)
])

test_transforms = v2.Compose([
    v2.Resize(size=(48,48)),
    # v2.Grayscale(num_output_channels=3),
    v2.ToTensor(),
    v2.ToDtype(dtype=torch.float32, scale=True),
    v2.RandomHorizontalFlip(p=0.5),
    v2.RandomVerticalFlip(p=0.5)
])

resize = v2.Resize(size=(48,48))

train_dataset = ImageDataset(
    annotations_file= "/content/archive_labels.csv",
    img_dir= "/content/images",
    split= "train",
    transform= train_transforms
)

test_dataset = ImageDataset(
    annotations_file= "/content/archive_labels.csv",
    img_dir= "/content/images",
    split= "test",
    transform= test_transforms
)

print("Total: ", len(pd.read_csv("/content/archive_labels.csv")))
print("Tamaño del conjunto de entrenamiento:", len(train_dataset))
print("Tamaño del conjunto de prueba:", len(test_dataset))

#Generamos los dataloaders
from torch.utils.data import DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
    num_workers=2
)

# DataLoader para el conjunto de prueba
test_loader = DataLoader(
    test_dataset,
    batch_size=32,
    shuffle=False,
    num_workers=2
)

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# Ya tienes esta versión mejorada en tu código más reciente, úsala:
class FERModel(nn.Module):
        def __init__(self):
            super(FERModel, self).__init__()
            self.conv_layers = nn.Sequential(
                nn.Conv2d(1, 32, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.BatchNorm2d(32), # Añadido Batch Normalization
                nn.MaxPool2d(2, 2),
                nn.Conv2d(32, 64, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.BatchNorm2d(64), # Añadido Batch Normalization
                nn.MaxPool2d(2, 2),
                nn.Conv2d(64, 128, kernel_size=3, padding=1), # Añadida otra capa convolucional
                nn.ReLU(),
                nn.BatchNorm2d(128), # Añadido Batch Normalization
                nn.MaxPool2d(2, 2)
            )
            self.fc_layers = nn.Sequential(
                nn.Linear(128 * 6 * 6, 256), # Ajustar el tamaño de entrada y salida según las capas convolucionales
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(128, 7)
            )

        def forward(self, x):
            x = self.conv_layers(x)
            x = x.view(x.size(0), -1)
            x = self.fc_layers(x)
            return x

model = FERModel()

model = FERModel()

def train_model(model, train_loader, val_loader, optimizer, num_epochs=10, device='cpu'):
    model = model.to(device)
    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = F.cross_entropy(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        epoch_train_loss = running_loss / len(train_loader.dataset)
        epoch_train_acc = correct / total
        train_losses.append(epoch_train_loss)
        train_accuracies.append(epoch_train_acc)

        # Evaluación
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = F.cross_entropy(outputs, labels)
                val_loss += loss.item() * images.size(0)

                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        epoch_val_loss = val_loss / len(val_loader.dataset)
        val_accuracy = correct / total

        val_losses.append(epoch_val_loss)
        val_accuracies.append(val_accuracy)

        print(f"Epoch {epoch+1}/{num_epochs} | "
              f"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | "
              f"Val Loss: {epoch_val_loss:.4f} | Val Acc: {val_accuracy:.4f}")

    # Graficar pérdidas y accuracy
    plt.figure(figsize=(12,5))

    plt.subplot(1,2,1)
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Training and Validation Loss")

    plt.subplot(1,2,2)
    plt.plot(train_accuracies, label="Train Accuracy", color='blue')
    plt.plot(val_accuracies, label="Validation Accuracy", color='green')
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Train & Validation Accuracy")
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Guardar modelo
    torch.save(model.state_dict(), 'modelo_entrenado.pth')

    return train_accuracies, val_accuracies

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FERModel()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

train_model(model, train_loader, test_loader, optimizer, num_epochs=50, device=device)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Etiquetas del FER2013
class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

# Recolección de verdaderos y predichos
y_true = []
y_pred = []

model.eval()
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Matriz de confusión
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)

plt.figure(figsize=(8, 6))
disp.plot(cmap='Blues', xticks_rotation=45)
plt.title("Matriz de Confusión")
plt.show()